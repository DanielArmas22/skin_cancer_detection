{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-01T20:37:26.544193Z",
     "iopub.status.busy": "2025-07-01T20:37:26.543972Z",
     "iopub.status.idle": "2025-07-01T20:37:48.256427Z",
     "shell.execute_reply": "2025-07-01T20:37:48.255738Z",
     "shell.execute_reply.started": "2025-07-01T20:37:26.544167Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.72.0rc1)\n",
      "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.1.0,>=1.26.0->tensorflow) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.1.0,>=1.26.0->tensorflow) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.23.2->pandas) (2.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.23.2->pandas) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.23.2->pandas) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.23.2->pandas) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.23.2->pandas) (2024.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-01 20:37:35.345876: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751402255.547998      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751402255.610166      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "GPU disponible: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "GPU activa: True\n"
     ]
    }
   ],
   "source": [
    "# Instalar dependencias\n",
    "!pip install tensorflow\n",
    "!pip install pandas matplotlib seaborn scikit-learn pillow\n",
    "\n",
    "# Verificar GPU\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU disponible: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"GPU activa: {tf.test.is_built_with_cuda()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T20:39:04.625352Z",
     "iopub.status.busy": "2025-07-01T20:39:04.625075Z",
     "iopub.status.idle": "2025-07-01T20:39:05.111503Z",
     "shell.execute_reply": "2025-07-01T20:39:05.110777Z",
     "shell.execute_reply.started": "2025-07-01T20:39:04.625326Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Todas las dependencias importadas correctamente\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import EfficientNetB4, ResNet152\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(\"✅ Todas las dependencias importadas correctamente\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T20:39:09.981885Z",
     "iopub.status.busy": "2025-07-01T20:39:09.980937Z",
     "iopub.status.idle": "2025-07-01T20:42:49.930522Z",
     "shell.execute_reply": "2025-07-01T20:42:49.929966Z",
     "shell.execute_reply.started": "2025-07-01T20:39:09.981856Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Cargando dataset ISIC 2019 desde Kaggle...\n",
      "📂 Etiquetas copiadas a data/ISIC_2019_Labels.csv\n",
      "📂 Copiando imágenes a carpeta local (esto puede tardar)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Ejecutar función\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mload_isic_dataset_from_kaggle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mload_isic_dataset_from_kaggle\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     32\u001b[39m         src_file = os.path.join(images_src_path, filename)\n\u001b[32m     33\u001b[39m         dst_file = os.path.join(\u001b[33m'\u001b[39m\u001b[33mdata/ISIC_2019_Images\u001b[39m\u001b[33m'\u001b[39m, filename)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m         \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Imágenes copiadas exitosamente!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:419\u001b[39m, in \u001b[36mcopy\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isdir(dst):\n\u001b[32m    418\u001b[39m     dst = os.path.join(dst, os.path.basename(src))\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m \u001b[43mcopyfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_symlinks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    420\u001b[39m copymode(src, dst, follow_symlinks=follow_symlinks)\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m dst\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:276\u001b[39m, in \u001b[36mcopyfile\u001b[39m\u001b[34m(src, dst, follow_symlinks)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# Windows, see:\u001b[39;00m\n\u001b[32m    274\u001b[39m \u001b[38;5;66;03m# https://github.com/python/cpython/pull/7160#discussion_r195405230\u001b[39;00m\n\u001b[32m    275\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m _WINDOWS \u001b[38;5;129;01mand\u001b[39;00m file_size > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[43m_copyfileobj_readinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfsrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfdst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCOPY_BUFSIZE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m dst\n\u001b[32m    279\u001b[39m copyfileobj(fsrc, fdst)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\shutil.py:180\u001b[39m, in \u001b[36m_copyfileobj_readinto\u001b[39m\u001b[34m(fsrc, fdst, length)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(\u001b[38;5;28mbytearray\u001b[39m(length)) \u001b[38;5;28;01mas\u001b[39;00m mv:\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         n = fsrc_readinto(mv)\n\u001b[32m    181\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m n:\n\u001b[32m    182\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "def load_isic_dataset_from_kaggle():\n",
    "    \"\"\"Carga el dataset ISIC 2019 desde el dataset privado en Kaggle con estructura anidada\"\"\"\n",
    "    print(\"🚀 Cargando dataset ISIC 2019 desde Kaggle...\")\n",
    "\n",
    "    base_path = 'dataset'\n",
    "\n",
    "    # Ruta del CSV de etiquetas\n",
    "    labels_path = os.path.join(base_path, 'ISIC_2019_Labels.csv')\n",
    "\n",
    "    # Ruta de la carpeta que contiene las imágenes (ya descomprimidas)\n",
    "    images_src_path = os.path.join(base_path, 'ISIC_2019_Training_Input')\n",
    "\n",
    "    # Crear carpeta local para etiquetas e imágenes\n",
    "    os.makedirs('data/ISIC_2019_Images', exist_ok=True)\n",
    "\n",
    "    # Copiar etiquetas CSV a carpeta local\n",
    "    if os.path.exists(labels_path):\n",
    "        shutil.copy(labels_path, 'data/ISIC_2019_Labels.csv')\n",
    "        print(\"📂 Etiquetas copiadas a data/ISIC_2019_Labels.csv\")\n",
    "    else:\n",
    "        print(f\"❌ No se encontró el archivo de etiquetas: {labels_path}\")\n",
    "\n",
    "    # Copiar imágenes a carpeta local (en lugar de extraer ZIP, ya están descomprimidas)\n",
    "    if os.path.exists(images_src_path):\n",
    "        print(\"📂 Copiando imágenes a carpeta local (esto puede tardar)...\")\n",
    "        # Copiar todos los archivos de imágenes\n",
    "        for filename in os.listdir(images_src_path):\n",
    "            src_file = os.path.join(images_src_path, filename)\n",
    "            dst_file = os.path.join('data/ISIC_2019_Images', filename)\n",
    "            shutil.copy(src_file, dst_file)\n",
    "        print(\"✅ Imágenes copiadas exitosamente!\")\n",
    "    else:\n",
    "        print(f\"❌ No se encontró la carpeta de imágenes: {images_src_path}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "# Ejecutar función\n",
    "load_isic_dataset_from_kaggle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Cargando dataset ISIC 2019 desde Kaggle...\n",
      "📂 Etiquetas copiadas a data/ISIC_2019_Labels.csv\n",
      "📂 Copiando imágenes a carpeta local (esto puede tardar)...\n",
      "✅ Imágenes copiadas exitosamente!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "def load_isic_dataset_from_kaggle():\n",
    "    \"\"\"Carga el dataset ISIC 2019 desde el dataset privado en Kaggle con estructura anidada\"\"\"\n",
    "    print(\"🚀 Cargando dataset ISIC 2019 desde Kaggle...\")\n",
    "\n",
    "    base_path = 'dataset'\n",
    "\n",
    "    # Ruta del CSV de etiquetas\n",
    "    labels_path = os.path.join(base_path, 'ISIC_2019_Labels.csv')\n",
    "\n",
    "    # Ruta de la carpeta que contiene las imágenes (ya descomprimidas)\n",
    "    images_src_path = os.path.join(base_path, 'ISIC_2019_Training_Input', 'ISIC_2019_Training_Input')\n",
    "\n",
    "    # Crear carpeta local para etiquetas e imágenes\n",
    "    os.makedirs('data/ISIC_2019_Images', exist_ok=True)\n",
    "\n",
    "    # Copiar etiquetas CSV a carpeta local\n",
    "    if os.path.exists(labels_path):\n",
    "        shutil.copy(labels_path, 'data/ISIC_2019_Labels.csv')\n",
    "        print(\"📂 Etiquetas copiadas a data/ISIC_2019_Labels.csv\")\n",
    "    else:\n",
    "        print(f\"❌ No se encontró el archivo de etiquetas: {labels_path}\")\n",
    "\n",
    "    # Copiar imágenes a carpeta local (en lugar de extraer ZIP, ya están descomprimidas)\n",
    "    if os.path.exists(images_src_path):\n",
    "        print(\"📂 Copiando imágenes a carpeta local (esto puede tardar)...\")\n",
    "        # Copiar todos los archivos de imágenes\n",
    "        for filename in os.listdir(images_src_path):\n",
    "            src_file = os.path.join(images_src_path, filename)\n",
    "            dst_file = os.path.join('data/ISIC_2019_Images', filename)\n",
    "            shutil.copy(src_file, dst_file)\n",
    "        print(\"✅ Imágenes copiadas exitosamente!\")\n",
    "    else:\n",
    "        print(f\"❌ No se encontró la carpeta de imágenes: {images_src_path}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "# Ejecutar función\n",
    "load_isic_dataset_from_kaggle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T20:43:30.804263Z",
     "iopub.status.busy": "2025-07-01T20:43:30.803702Z",
     "iopub.status.idle": "2025-07-01T20:43:30.824916Z",
     "shell.execute_reply": "2025-07-01T20:43:30.824323Z",
     "shell.execute_reply.started": "2025-07-01T20:43:30.804238Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de imágenes en 'dataset/ISIC_2019_Images': 25331\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "image_dir = 'dataset/ISIC_2019_Images'\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "image_files = [f for f in os.listdir(image_dir) if f.lower().endswith(valid_extensions)]\n",
    "print(f\"Número de imágenes en '{image_dir}': {len(image_files)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T20:43:34.940941Z",
     "iopub.status.busy": "2025-07-01T20:43:34.940662Z",
     "iopub.status.idle": "2025-07-01T20:44:32.792764Z",
     "shell.execute_reply": "2025-07-01T20:44:32.792016Z",
     "shell.execute_reply.started": "2025-07-01T20:43:34.940920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Organizando dataset...\n",
      "📊 Total de imágenes: 25331\n",
      "✅ Dataset organizado:\n",
      "   Train benign: 13497 imágenes\n",
      "   Train malignant: 6300 imágenes\n",
      "   Test benign: 3361 imágenes\n",
      "   Test malignant: 1545 imágenes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def organize_isic_data():\n",
    "    \"\"\"Organiza el dataset ISIC en estructura train/test en Kaggle\"\"\"\n",
    "    print(\"📁 Organizando dataset...\")\n",
    "\n",
    "    # Verifica si las imágenes existen\n",
    "    if not Path('dataset/ISIC_2019_Images').exists():\n",
    "        raise FileNotFoundError(\"❌ No se encontró la carpeta con las imágenes. Asegúrate de haber ejecutado la carga correctamente.\")\n",
    "\n",
    "    # Leer etiquetas\n",
    "    labels_df = pd.read_csv('dataset/ISIC_2019_Labels.csv')\n",
    "    print(f\"📊 Total de imágenes: {len(labels_df)}\")\n",
    "\n",
    "    # Crear estructura de directorios\n",
    "    train_dir = Path('data/ISIC_dataset')\n",
    "    test_dir = Path('data/ISIC_dataset_test')\n",
    "\n",
    "    train_dir.mkdir(exist_ok=True)\n",
    "    test_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Crear subdirectorios\n",
    "    (train_dir / 'benign').mkdir(exist_ok=True)\n",
    "    (train_dir / 'malignant').mkdir(exist_ok=True)\n",
    "    (test_dir / 'benign').mkdir(exist_ok=True)\n",
    "    (test_dir / 'malignant').mkdir(exist_ok=True)\n",
    "\n",
    "    # Semilla\n",
    "    random.seed(42)\n",
    "\n",
    "    # Contadores\n",
    "    train_benign = 0\n",
    "    train_malignant = 0\n",
    "    test_benign = 0\n",
    "    test_malignant = 0\n",
    "\n",
    "    # Procesar cada imagen\n",
    "    for index, row in labels_df.iterrows():\n",
    "        image_name = row['image']\n",
    "\n",
    "        is_malignant = float(row.get('MEL', 0)) == 1.0 or float(row.get('BCC', 0)) == 1.0\n",
    "        is_benign = (\n",
    "            float(row.get('NV', 0)) == 1.0 or\n",
    "            float(row.get('BKL', 0)) == 1.0 or\n",
    "            float(row.get('AK', 0)) == 1.0 or\n",
    "            float(row.get('VASC', 0)) == 1.0 or\n",
    "            float(row.get('DF', 0)) == 1.0\n",
    "        )\n",
    "\n",
    "        if is_malignant:\n",
    "            class_name = 'malignant'\n",
    "        elif is_benign:\n",
    "            class_name = 'benign'\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        source_path = Path('dataset/ISIC_2019_Images') / f\"{image_name}.jpg\"\n",
    "\n",
    "        if source_path.exists():\n",
    "            # 80% entrenamiento, 20% validación\n",
    "            if random.random() < 0.8:\n",
    "                dest_path = train_dir / class_name / f\"{image_name}.jpg\"\n",
    "                if class_name == 'benign':\n",
    "                    train_benign += 1\n",
    "                else:\n",
    "                    train_malignant += 1\n",
    "            else:\n",
    "                dest_path = test_dir / class_name / f\"{image_name}.jpg\"\n",
    "                if class_name == 'benign':\n",
    "                    test_benign += 1\n",
    "                else:\n",
    "                    test_malignant += 1\n",
    "\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "\n",
    "    print(f\"✅ Dataset organizado:\")\n",
    "    print(f\"   Train benign: {train_benign} imágenes\")\n",
    "    print(f\"   Train malignant: {train_malignant} imágenes\")\n",
    "    print(f\"   Test benign: {test_benign} imágenes\")\n",
    "    print(f\"   Test malignant: {test_malignant} imágenes\")\n",
    "\n",
    "    # ❌ No borrar archivos al final, ya que se cargaron desde /kaggle/input\n",
    "    # os.remove(...)\n",
    "    # shutil.rmtree(...)\n",
    "\n",
    "    return True\n",
    "\n",
    "# Ejecutar\n",
    "organize_isic_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T20:45:56.071312Z",
     "iopub.status.busy": "2025-07-01T20:45:56.070549Z",
     "iopub.status.idle": "2025-07-01T20:45:57.416540Z",
     "shell.execute_reply": "2025-07-01T20:45:57.412978Z",
     "shell.execute_reply.started": "2025-07-01T20:45:56.071283Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Carpeta 'data/ISIC_2019_Images' eliminada para liberar espacio.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Elimina las imágenes copiadas (originales)\n",
    "shutil.rmtree('data/ISIC_2019_Images')\n",
    "print(\"🗑️ Carpeta 'data/ISIC_2019_Images' eliminada para liberar espacio.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T20:46:03.694186Z",
     "iopub.status.busy": "2025-07-01T20:46:03.693916Z",
     "iopub.status.idle": "2025-07-01T20:46:13.518846Z",
     "shell.execute_reply": "2025-07-01T20:46:13.518205Z",
     "shell.execute_reply.started": "2025-07-01T20:46:03.694163Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Creando modelo EfficientNetB4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1751402764.088294      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
      "I0000 00:00:1751402764.090085      35 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb4_notop.h5\n",
      "\u001b[1m71686520/71686520\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "🚀 Creando modelo ResNet152...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "\u001b[1m234698864/234698864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "🚀 Creando modelo CNN Personalizada...\n",
      "✅ Modelos creados exitosamente!\n"
     ]
    }
   ],
   "source": [
    "def create_models():\n",
    "    \"\"\"Crea los 3 modelos para entrenamiento\"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Modelo 1: EfficientNetB4\n",
    "    print(\"🔧 Creando modelo EfficientNetB4...\")\n",
    "    base_model = EfficientNetB4(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(300, 300, 3)\n",
    "    )\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    efficientnet = Model(inputs=base_model.input, outputs=predictions)\n",
    "    efficientnet.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    models['EfficientNetB4'] = efficientnet\n",
    "    \n",
    "    # Modelo 2: ResNet152\n",
    "    print(\"🚀 Creando modelo ResNet152...\")\n",
    "    base_model = ResNet152(\n",
    "        weights='imagenet',\n",
    "        include_top=False,\n",
    "        input_shape=(300, 300, 3)\n",
    "    )\n",
    "    base_model.trainable = True\n",
    "    \n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    predictions = Dense(1, activation='sigmoid')(x)\n",
    "    resnet = Model(inputs=base_model.input, outputs=predictions)\n",
    "    resnet.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    models['ResNet152'] = resnet\n",
    "    \n",
    "    # Modelo 3: CNN Personalizada\n",
    "    print(\"🚀 Creando modelo CNN Personalizada...\")\n",
    "    cnn_personalizada = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(300, 300, 3)),\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "        tf.keras.layers.MaxPooling2D(2,2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    cnn_personalizada.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    models['CNN Personalizada'] = cnn_personalizada\n",
    "    \n",
    "    print(\"✅ Modelos creados exitosamente!\")\n",
    "    return models\n",
    "\n",
    "# Crear modelos\n",
    "models = create_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T20:46:38.545235Z",
     "iopub.status.busy": "2025-07-01T20:46:38.544370Z",
     "iopub.status.idle": "2025-07-01T20:46:39.017941Z",
     "shell.execute_reply": "2025-07-01T20:46:39.017320Z",
     "shell.execute_reply.started": "2025-07-01T20:46:38.545201Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Preparando datos...\n",
      "Found 19797 images belonging to 2 classes.\n",
      "Found 4906 images belonging to 2 classes.\n",
      "✅ Datos preparados:\n",
      "   Train: 19797 muestras\n",
      "   Validation: 4906 muestras\n",
      "   Batch size: 32\n"
     ]
    }
   ],
   "source": [
    "def prepare_data():\n",
    "    \"\"\"Prepara los datos para entrenamiento\"\"\"\n",
    "    print(\"📊 Preparando datos...\")\n",
    "    \n",
    "    # Data augmentation para entrenamiento\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    # Solo rescaling para validación\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    # Cargar datos\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        'data/ISIC_dataset',\n",
    "        target_size=(300, 300),\n",
    "        batch_size=32,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    \n",
    "    val_generator = val_datagen.flow_from_directory(\n",
    "        'data/ISIC_dataset_test',\n",
    "        target_size=(300, 300),\n",
    "        batch_size=32,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Datos preparados:\")\n",
    "    print(f\"   Train: {train_generator.samples} muestras\")\n",
    "    print(f\"   Validation: {val_generator.samples} muestras\")\n",
    "    print(f\"   Batch size: 32\")\n",
    "    \n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Preparar datos\n",
    "train_gen, val_gen = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T20:47:04.804911Z",
     "iopub.status.busy": "2025-07-01T20:47:04.804623Z",
     "iopub.status.idle": "2025-07-01T20:47:04.819954Z",
     "shell.execute_reply": "2025-07-01T20:47:04.819367Z",
     "shell.execute_reply.started": "2025-07-01T20:47:04.804890Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Class weights: {0: 0.73338519671038, 1: 1.5711904761904762}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Calcular class weights a partir de los datos de entrenamiento\n",
    "labels = train_gen.classes  # 0 = benign, 1 = malignant\n",
    "class_weights_array = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(labels),\n",
    "    y=labels\n",
    ")\n",
    "class_weights = dict(enumerate(class_weights_array))\n",
    "print(f\"📊 Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T20:48:19.901612Z",
     "iopub.status.busy": "2025-07-01T20:48:19.901018Z",
     "iopub.status.idle": "2025-07-01T21:26:37.463795Z",
     "shell.execute_reply": "2025-07-01T21:26:37.463158Z",
     "shell.execute_reply.started": "2025-07-01T20:48:19.901583Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🚀 Entrenando modelo: EfficientNetB4\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1751403012.150780     110 service.cc:148] XLA service 0x7ba7e4004270 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1751403012.152019     110 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1751403012.152043     110 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n",
      "I0000 00:00:1751403022.382762     110 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "E0000 00:00:1751403046.131288     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403046.283388     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403046.642930     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403046.795190     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403047.277543     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403047.440257     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403048.081793     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403048.235986     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403048.686505     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403048.840866     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403049.603327     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403049.787025     110 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "I0000 00:00:1751403088.630484     110 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m240/618\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5:52\u001b[0m 932ms/step - accuracy: 0.6606 - loss: 0.6389 - precision: 0.4778 - recall: 0.6611"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1751403340.028059     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403340.172455     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403340.558581     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403340.702759     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403341.093235     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403341.245005     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403341.867854     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403342.022372     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403342.345900     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403342.500773     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403342.996726     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1751403343.180923     112 gpu_timer.cc:82] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - accuracy: 0.6892 - loss: 0.5866 - precision: 0.5108 - recall: 0.7035\n",
      "Epoch 1: val_accuracy improved from -inf to 0.68546, saving model to models/efficientnetb4.h5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m894s\u001b[0m 1s/step - accuracy: 0.6892 - loss: 0.5865 - precision: 0.5108 - recall: 0.7036 - val_accuracy: 0.6855 - val_loss: 0.6092 - val_precision: 0.7500 - val_recall: 0.0039 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:56\u001b[0m 773ms/step - accuracy: 0.7812 - loss: 0.5703 - precision: 0.5714 - recall: 0.8889"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy improved from 0.68546 to 0.68566, saving model to models/efficientnetb4.h5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 84ms/step - accuracy: 0.7812 - loss: 0.5703 - precision: 0.5714 - recall: 0.8889 - val_accuracy: 0.6857 - val_loss: 0.6114 - val_precision: 1.0000 - val_recall: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 866ms/step - accuracy: 0.7711 - loss: 0.4805 - precision: 0.6109 - recall: 0.7753\n",
      "Epoch 3: val_accuracy did not improve from 0.68566\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m587s\u001b[0m 949ms/step - accuracy: 0.7711 - loss: 0.4805 - precision: 0.6109 - recall: 0.7753 - val_accuracy: 0.6846 - val_loss: 0.8072 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:36\u001b[0m 740ms/step - accuracy: 0.9062 - loss: 0.2931 - precision: 0.9000 - recall: 0.8182\n",
      "Epoch 4: val_accuracy did not improve from 0.68566\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 83ms/step - accuracy: 0.9062 - loss: 0.2931 - precision: 0.9000 - recall: 0.8182 - val_accuracy: 0.6853 - val_loss: 0.7610 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 860ms/step - accuracy: 0.8151 - loss: 0.4027 - precision: 0.6814 - recall: 0.8120\n",
      "Epoch 5: val_accuracy improved from 0.68566 to 0.69158, saving model to models/efficientnetb4.h5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m584s\u001b[0m 944ms/step - accuracy: 0.8151 - loss: 0.4026 - precision: 0.6814 - recall: 0.8120 - val_accuracy: 0.6916 - val_loss: 0.7743 - val_precision: 0.8667 - val_recall: 0.0253 - learning_rate: 2.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7:40\u001b[0m 746ms/step - accuracy: 0.9375 - loss: 0.3571 - precision: 1.0000 - recall: 0.8571\n",
      "Epoch 6: val_accuracy improved from 0.69158 to 0.69383, saving model to models/efficientnetb4.h5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 86ms/step - accuracy: 0.9375 - loss: 0.3571 - precision: 1.0000 - recall: 0.8571 - val_accuracy: 0.6938 - val_loss: 0.7463 - val_precision: 0.8525 - val_recall: 0.0337 - learning_rate: 2.0000e-04\n",
      "Epoch 6: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "✅ Resultados finales de EfficientNetB4:\n",
      "   Accuracy: 0.6859\n",
      "   Precision: 0.7500\n",
      "   Recall: 0.0039\n",
      "📦 Modelo comprimido en: efficientnetb4.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def train_and_export_single_model(model_name, models, train_gen, val_gen, class_weights):\n",
    "    \"\"\"Entrena un único modelo y lo exporta comprimido en ZIP\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🚀 Entrenando modelo: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Epochs por tipo de modelo\n",
    "    epochs = 30 if \"CNN Personalizada\" in model_name else 20\n",
    "\n",
    "    # Asegurar carpeta de modelos\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'models/{model_name.lower().replace(\" \", \"_\")}.h5',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Entrenamiento\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=train_gen.samples // train_gen.batch_size,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=val_gen.samples // val_gen.batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluación\n",
    "    val_loss, val_acc, val_prec, val_rec = model.evaluate(val_gen, verbose=0)\n",
    "    print(f\"\\n✅ Resultados finales de {model_name}:\")\n",
    "    print(f\"   Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"   Precision: {val_prec:.4f}\")\n",
    "    print(f\"   Recall: {val_rec:.4f}\")\n",
    "\n",
    "    # Comprimir modelo\n",
    "    model_file = f'models/{model_name.lower().replace(\" \", \"_\")}.h5'\n",
    "    zip_file = f'{model_name.lower().replace(\" \", \"_\")}.zip'\n",
    "    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(model_file)\n",
    "\n",
    "    print(f\"📦 Modelo comprimido en: {zip_file}\")\n",
    "    \n",
    "train_and_export_single_model('EfficientNetB4', models, train_gen, val_gen, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-01T21:31:48.715161Z",
     "iopub.status.busy": "2025-07-01T21:31:48.714852Z",
     "iopub.status.idle": "2025-07-02T00:48:21.186502Z",
     "shell.execute_reply": "2025-07-02T00:48:21.185762Z",
     "shell.execute_reply.started": "2025-07-01T21:31:48.715141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "🚀 Entrenando modelo: ResNet152\n",
      "============================================================\n",
      "Epoch 1/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5748 - loss: 0.7266 - precision: 0.3945 - recall: 0.6675\n",
      "Epoch 1: val_accuracy improved from -inf to 0.68505, saving model to models/resnet152.h5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1362s\u001b[0m 2s/step - accuracy: 0.5748 - loss: 0.7264 - precision: 0.3945 - recall: 0.6676 - val_accuracy: 0.6850 - val_loss: 0.6019 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - learning_rate: 0.0010\n",
      "Epoch 2/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15:47\u001b[0m 2s/step - accuracy: 0.6562 - loss: 0.6001 - precision: 0.5000 - recall: 0.8182\n",
      "Epoch 2: val_accuracy did not improve from 0.68505\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 126ms/step - accuracy: 0.6562 - loss: 0.6001 - precision: 0.5000 - recall: 0.8182 - val_accuracy: 0.6840 - val_loss: 0.6080 - val_precision: 0.2000 - val_recall: 6.4767e-04 - learning_rate: 0.0010\n",
      "Epoch 3/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6006 - loss: 0.6287 - precision: 0.4238 - recall: 0.7052\n",
      "Epoch 3: val_accuracy did not improve from 0.68505\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1061s\u001b[0m 2s/step - accuracy: 0.6006 - loss: 0.6287 - precision: 0.4238 - recall: 0.7052 - val_accuracy: 0.5976 - val_loss: 0.6994 - val_precision: 0.4216 - val_recall: 0.7464 - learning_rate: 0.0010\n",
      "Epoch 4/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:09\u001b[0m 2s/step - accuracy: 0.7500 - loss: 0.5993 - precision: 0.6250 - recall: 0.8333\n",
      "Epoch 4: val_accuracy did not improve from 0.68505\n",
      "\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 127ms/step - accuracy: 0.7500 - loss: 0.5993 - precision: 0.6250 - recall: 0.8333 - val_accuracy: 0.5670 - val_loss: 0.8592 - val_precision: 0.4037 - val_recall: 0.7896 - learning_rate: 0.0010\n",
      "Epoch 5/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.5983 - loss: 0.5986 - precision: 0.4272 - recall: 0.7466\n",
      "Epoch 5: val_accuracy did not improve from 0.68505\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1058s\u001b[0m 2s/step - accuracy: 0.5983 - loss: 0.5986 - precision: 0.4272 - recall: 0.7467 - val_accuracy: 0.6511 - val_loss: 0.5777 - val_precision: 0.4635 - val_recall: 0.6835 - learning_rate: 2.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:00\u001b[0m 2s/step - accuracy: 0.6562 - loss: 0.6685 - precision: 0.5556 - recall: 0.7692\n",
      "Epoch 6: val_accuracy did not improve from 0.68505\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 126ms/step - accuracy: 0.6562 - loss: 0.6685 - precision: 0.5556 - recall: 0.7692 - val_accuracy: 0.6509 - val_loss: 0.5784 - val_precision: 0.4631 - val_recall: 0.6840 - learning_rate: 2.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6243 - loss: 0.5894 - precision: 0.4468 - recall: 0.7420\n",
      "Epoch 7: val_accuracy did not improve from 0.68505\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1054s\u001b[0m 2s/step - accuracy: 0.6243 - loss: 0.5894 - precision: 0.4468 - recall: 0.7420 - val_accuracy: 0.6614 - val_loss: 0.5808 - val_precision: 0.4742 - val_recall: 0.6900 - learning_rate: 2.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:05\u001b[0m 2s/step - accuracy: 0.4688 - loss: 0.6324 - precision: 0.2222 - recall: 0.5714\n",
      "Epoch 8: val_accuracy did not improve from 0.68505\n",
      "\n",
      "Epoch 8: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 126ms/step - accuracy: 0.4688 - loss: 0.6324 - precision: 0.2222 - recall: 0.5714 - val_accuracy: 0.6554 - val_loss: 0.5814 - val_precision: 0.4684 - val_recall: 0.6965 - learning_rate: 2.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6516 - loss: 0.5746 - precision: 0.4726 - recall: 0.7506\n",
      "Epoch 9: val_accuracy did not improve from 0.68505\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1055s\u001b[0m 2s/step - accuracy: 0.6516 - loss: 0.5746 - precision: 0.4726 - recall: 0.7506 - val_accuracy: 0.6806 - val_loss: 0.5653 - val_precision: 0.4950 - val_recall: 0.6399 - learning_rate: 4.0000e-05\n",
      "Epoch 10/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:09\u001b[0m 2s/step - accuracy: 0.4375 - loss: 0.7288 - precision: 0.2000 - recall: 0.3333\n",
      "Epoch 10: val_accuracy did not improve from 0.68505\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 128ms/step - accuracy: 0.4375 - loss: 0.7288 - precision: 0.2000 - recall: 0.3333 - val_accuracy: 0.6804 - val_loss: 0.5648 - val_precision: 0.4948 - val_recall: 0.6457 - learning_rate: 4.0000e-05\n",
      "Epoch 11/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6759 - loss: 0.5632 - precision: 0.5025 - recall: 0.7404\n",
      "Epoch 11: val_accuracy improved from 0.68505 to 0.69077, saving model to models/resnet152.h5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1060s\u001b[0m 2s/step - accuracy: 0.6759 - loss: 0.5632 - precision: 0.5025 - recall: 0.7404 - val_accuracy: 0.6908 - val_loss: 0.5454 - val_precision: 0.5061 - val_recall: 0.6456 - learning_rate: 4.0000e-05\n",
      "Epoch 12/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15:59\u001b[0m 2s/step - accuracy: 0.6250 - loss: 0.6162 - precision: 0.3750 - recall: 0.7500\n",
      "Epoch 12: val_accuracy did not improve from 0.69077\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 127ms/step - accuracy: 0.6250 - loss: 0.6162 - precision: 0.3750 - recall: 0.7500 - val_accuracy: 0.6895 - val_loss: 0.5462 - val_precision: 0.5056 - val_recall: 0.6472 - learning_rate: 4.0000e-05\n",
      "Epoch 13/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6762 - loss: 0.5561 - precision: 0.4919 - recall: 0.7288\n",
      "Epoch 13: val_accuracy did not improve from 0.69077\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1055s\u001b[0m 2s/step - accuracy: 0.6763 - loss: 0.5561 - precision: 0.4919 - recall: 0.7288 - val_accuracy: 0.6744 - val_loss: 0.5689 - val_precision: 0.4888 - val_recall: 0.7361 - learning_rate: 4.0000e-05\n",
      "Epoch 14/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:05\u001b[0m 2s/step - accuracy: 0.6562 - loss: 0.4419 - precision: 0.2308 - recall: 0.7500\n",
      "Epoch 14: val_accuracy did not improve from 0.69077\n",
      "\n",
      "Epoch 14: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 126ms/step - accuracy: 0.6562 - loss: 0.4419 - precision: 0.2308 - recall: 0.7500 - val_accuracy: 0.6759 - val_loss: 0.5669 - val_precision: 0.4902 - val_recall: 0.7289 - learning_rate: 4.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6884 - loss: 0.5427 - precision: 0.5071 - recall: 0.7584\n",
      "Epoch 15: val_accuracy did not improve from 0.69077\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1055s\u001b[0m 2s/step - accuracy: 0.6884 - loss: 0.5427 - precision: 0.5071 - recall: 0.7584 - val_accuracy: 0.6906 - val_loss: 0.5452 - val_precision: 0.5063 - val_recall: 0.6814 - learning_rate: 8.0000e-06\n",
      "Epoch 16/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:07\u001b[0m 2s/step - accuracy: 0.6875 - loss: 0.5816 - precision: 0.6316 - recall: 0.8000\n",
      "Epoch 16: val_accuracy did not improve from 0.69077\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 128ms/step - accuracy: 0.6875 - loss: 0.5816 - precision: 0.6316 - recall: 0.8000 - val_accuracy: 0.6908 - val_loss: 0.5446 - val_precision: 0.5065 - val_recall: 0.6840 - learning_rate: 8.0000e-06\n",
      "Epoch 17/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6921 - loss: 0.5449 - precision: 0.5178 - recall: 0.7585\n",
      "Epoch 17: val_accuracy improved from 0.69077 to 0.69812, saving model to models/resnet152.h5\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1055s\u001b[0m 2s/step - accuracy: 0.6921 - loss: 0.5449 - precision: 0.5177 - recall: 0.7585 - val_accuracy: 0.6981 - val_loss: 0.5421 - val_precision: 0.5163 - val_recall: 0.6675 - learning_rate: 8.0000e-06\n",
      "Epoch 18/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:01\u001b[0m 2s/step - accuracy: 0.7500 - loss: 0.4393 - precision: 0.4615 - recall: 0.8571\n",
      "Epoch 18: val_accuracy did not improve from 0.69812\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 127ms/step - accuracy: 0.7500 - loss: 0.4393 - precision: 0.4615 - recall: 0.8571 - val_accuracy: 0.6981 - val_loss: 0.5423 - val_precision: 0.5155 - val_recall: 0.6688 - learning_rate: 8.0000e-06\n",
      "Epoch 19/20\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - accuracy: 0.6907 - loss: 0.5463 - precision: 0.5122 - recall: 0.7383\n",
      "Epoch 19: val_accuracy did not improve from 0.69812\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1054s\u001b[0m 2s/step - accuracy: 0.6907 - loss: 0.5463 - precision: 0.5122 - recall: 0.7383 - val_accuracy: 0.6914 - val_loss: 0.5425 - val_precision: 0.5071 - val_recall: 0.6957 - learning_rate: 8.0000e-06\n",
      "Epoch 20/20\n",
      "\u001b[1m  1/618\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16:07\u001b[0m 2s/step - accuracy: 0.5312 - loss: 0.6133 - precision: 0.3333 - recall: 0.5000\n",
      "Epoch 20: val_accuracy did not improve from 0.69812\n",
      "\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\u001b[1m618/618\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 128ms/step - accuracy: 0.5312 - loss: 0.6133 - precision: 0.3333 - recall: 0.5000 - val_accuracy: 0.6924 - val_loss: 0.5421 - val_precision: 0.5090 - val_recall: 0.6937 - learning_rate: 8.0000e-06\n",
      "Restoring model weights from the end of the best epoch: 20.\n",
      "\n",
      "✅ Resultados finales de ResNet152:\n",
      "   Accuracy: 0.6926\n",
      "   Precision: 0.5088\n",
      "   Recall: 0.6932\n",
      "📦 Modelo comprimido en: resnet152.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def train_and_export_single_model(model_name, models, train_gen, val_gen, class_weights):\n",
    "    \"\"\"Entrena un único modelo y lo exporta comprimido en ZIP\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🚀 Entrenando modelo: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Epochs por tipo de modelo\n",
    "    epochs = 30 if \"CNN Personalizada\" in model_name else 20\n",
    "\n",
    "    # Asegurar carpeta de modelos\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'models/{model_name.lower().replace(\" \", \"_\")}.h5',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Entrenamiento\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=train_gen.samples // train_gen.batch_size,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=val_gen.samples // val_gen.batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluación\n",
    "    val_loss, val_acc, val_prec, val_rec = model.evaluate(val_gen, verbose=0)\n",
    "    print(f\"\\n✅ Resultados finales de {model_name}:\")\n",
    "    print(f\"   Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"   Precision: {val_prec:.4f}\")\n",
    "    print(f\"   Recall: {val_rec:.4f}\")\n",
    "\n",
    "    # Comprimir modelo\n",
    "    model_file = f'models/{model_name.lower().replace(\" \", \"_\")}.h5'\n",
    "    zip_file = f'{model_name.lower().replace(\" \", \"_\")}.zip'\n",
    "    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(model_file)\n",
    "\n",
    "    print(f\"📦 Modelo comprimido en: {zip_file}\")\n",
    "    \n",
    "train_and_export_single_model('ResNet152', models, train_gen, val_gen, class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-02T01:08:02.628262Z",
     "iopub.status.busy": "2025-07-02T01:08:02.627976Z",
     "iopub.status.idle": "2025-07-02T03:27:55.206241Z",
     "shell.execute_reply": "2025-07-02T03:27:55.205557Z",
     "shell.execute_reply.started": "2025-07-02T01:08:02.628240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def train_and_export_single_model(model_name, models, train_gen, val_gen, class_weights):\n",
    "    \"\"\"Entrena un único modelo y lo exporta comprimido en ZIP\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"🚀 Entrenando modelo: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    model = models[model_name]\n",
    "    \n",
    "    # Epochs por tipo de modelo\n",
    "    epochs = 30 if \"CNN Personalizada\" in model_name else 20\n",
    "\n",
    "    # Asegurar carpeta de modelos\n",
    "    os.makedirs('models', exist_ok=True)\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(\n",
    "            filepath=f'models/{model_name.lower().replace(\" \", \"_\")}.h5',\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=5,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Entrenamiento\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        steps_per_epoch=train_gen.samples // train_gen.batch_size,\n",
    "        validation_data=val_gen,\n",
    "        validation_steps=val_gen.samples // val_gen.batch_size,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        class_weight=class_weights,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluación\n",
    "    val_loss, val_acc, val_prec, val_rec = model.evaluate(val_gen, verbose=0)\n",
    "    print(f\"\\n✅ Resultados finales de {model_name}:\")\n",
    "    print(f\"   Accuracy: {val_acc:.4f}\")\n",
    "    print(f\"   Precision: {val_prec:.4f}\")\n",
    "    print(f\"   Recall: {val_rec:.4f}\")\n",
    "\n",
    "    # Comprimir modelo\n",
    "    model_file = f'models/{model_name.lower().replace(\" \", \"_\")}.h5'\n",
    "    zip_file = f'{model_name.lower().replace(\" \", \"_\")}.zip'\n",
    "    with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(model_file)\n",
    "\n",
    "    print(f\"📦 Modelo comprimido en: {zip_file}\")\n",
    "    \n",
    "train_and_export_single_model('CNN Personalizada', models, train_gen, val_gen, class_weights)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7781458,
     "sourceId": 12343405,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
