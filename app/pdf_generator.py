from fpdf import FPDF
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime
import os

class PDFReportGenerator:
    """
    Generador de reportes PDF para evaluaci√≥n de modelos de c√°ncer de piel
    """
    
    def __init__(self, output_dir="reports"):
        """
        Inicializa el generador de reportes
        
        Args:
            output_dir: Directorio para guardar reportes
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        
        # Configurar PDF
        self.pdf = FPDF()
        self.pdf.add_page()
        self.pdf.set_font("Arial", size=12)
        
    def add_title_page(self, title="Reporte de Evaluaci√≥n de Modelos"):
        """
        A√±ade p√°gina de t√≠tulo
        
        Args:
            title: T√≠tulo del reporte
        """
        self.pdf.set_font("Arial", style="B", size=20)
        self.pdf.cell(0, 20, title, ln=True, align="C")
        
        self.pdf.ln(10)
        
        # Informaci√≥n del reporte
        self.pdf.set_font("Arial", size=14)
        self.pdf.cell(0, 10, "Sistema de Diagn√≥stico de C√°ncer de Piel", ln=True, align="C")
        
        self.pdf.ln(10)
        
        self.pdf.set_font("Arial", size=12)
        self.pdf.cell(0, 10, f"Fecha de generaci√≥n: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}", ln=True, align="C")
        
        self.pdf.ln(20)
        
        # Disclaimer
        self.pdf.set_font("Arial", style="B", size=14)
        self.pdf.cell(0, 10, "‚ö†Ô∏è DISCLAIMER M√âDICO", ln=True, align="C")
        
        self.pdf.ln(5)
        
        disclaimer_text = """
        Este sistema es solo para fines educativos y de investigaci√≥n.
        NO debe utilizarse para diagn√≥stico m√©dico real.
        Siempre consulte a un profesional m√©dico calificado.
        """
        
        self.pdf.set_font("Arial", size=10)
        lines = disclaimer_text.strip().split('\\n')
        for line in lines:
            self.pdf.cell(0, 6, line.strip(), ln=True, align="C")
    
    def add_section_title(self, title):
        """
        A√±ade t√≠tulo de secci√≥n
        
        Args:
            title: T√≠tulo de la secci√≥n
        """
        self.pdf.ln(10)
        self.pdf.set_font("Arial", style="B", size=14)
        self.pdf.cell(0, 10, title, ln=True)
        self.pdf.ln(5)
    
    def add_text(self, text, font_size=12):
        """
        A√±ade texto normal
        
        Args:
            text: Texto a a√±adir
            font_size: Tama√±o de fuente
        """
        self.pdf.set_font("Arial", size=font_size)
        
        # Dividir texto en l√≠neas que quepan en la p√°gina
        lines = text.split('\\n')
        for line in lines:
            # Manejar l√≠neas largas
            while len(line) > 80:
                break_point = 80
                while break_point > 0 and line[break_point] != ' ':
                    break_point -= 1
                if break_point == 0:
                    break_point = 80
                
                self.pdf.cell(0, 6, line[:break_point], ln=True)
                line = line[break_point:].lstrip()
            
            if line:
                self.pdf.cell(0, 6, line, ln=True)
        
        self.pdf.ln(5)
    
    def add_metrics_table(self, metrics_data):
        """
        A√±ade tabla de m√©tricas
        
        Args:
            metrics_data: Datos de m√©tricas de los modelos
        """
        self.add_section_title("üìä Resumen de M√©tricas")
        
        # Encabezados
        self.pdf.set_font("Arial", style="B", size=10)
        
        # Calcular anchos de columna
        col_widths = [40, 20, 20, 20, 20, 20, 20]
        headers = ["Modelo", "Accuracy", "Precision", "Recall", "F1-Score", "MCC", "AUC-ROC"]
        
        # A√±adir encabezados
        for i, header in enumerate(headers):
            self.pdf.cell(col_widths[i], 8, header, border=1, align="C")
        self.pdf.ln()
        
        # A√±adir datos
        self.pdf.set_font("Arial", size=9)
        
        # Ordenar por MCC
        sorted_models = sorted(metrics_data.items(), key=lambda x: x[1]['mcc'], reverse=True)
        
        for model_name, data in sorted_models:
            values = [
                model_name[:15],  # Truncar nombre si es muy largo
                f"{data['accuracy']:.3f}",
                f"{data['precision']:.3f}",
                f"{data['recall']:.3f}",
                f"{data['f1_score']:.3f}",
                f"{data['mcc']:.3f}",
                f"{data['auc_roc']:.3f}"
            ]
            
            for i, value in enumerate(values):
                self.pdf.cell(col_widths[i], 8, str(value), border=1, align="C")
            self.pdf.ln()
        
        self.pdf.ln(10)
    
    def add_best_model_analysis(self, metrics_data):
        """
        A√±ade an√°lisis del mejor modelo
        
        Args:
            metrics_data: Datos de m√©tricas
        """
        # Encontrar mejor modelo
        best_model = max(metrics_data.items(), key=lambda x: x[1]['mcc'])
        model_name, model_data = best_model
        
        self.add_section_title(f"üèÜ An√°lisis del Mejor Modelo: {model_name}")
        
        analysis_text = f"""
        Modelo con mejor rendimiento basado en Matthews Correlation Coefficient (MCC).
        
        M√©tricas principales:
        ‚Ä¢ Accuracy: {model_data['accuracy']:.3f} ({model_data['accuracy']*100:.1f}%)
        ‚Ä¢ Precision: {model_data['precision']:.3f}
        ‚Ä¢ Recall (Sensibilidad): {model_data['recall']:.3f}
        ‚Ä¢ F1-Score: {model_data['f1_score']:.3f}
        ‚Ä¢ MCC: {model_data['mcc']:.3f}
        ‚Ä¢ AUC-ROC: {model_data['auc_roc']:.3f}
        ‚Ä¢ Especificidad: {model_data['specificity']:.3f}
        
        Interpretaci√≥n:
        ‚Ä¢ Muestras evaluadas: {model_data['test_samples']:,}
        ‚Ä¢ Casos benignos: {model_data['class_distribution'][0]:,}
        ‚Ä¢ Casos malignos: {model_data['class_distribution'][1]:,}
        """
        
        self.add_text(analysis_text)
        
        # An√°lisis de matriz de confusi√≥n
        cm = model_data['confusion_matrix']
        tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]
        
        confusion_text = f"""
        Matriz de Confusi√≥n:
        ‚Ä¢ Verdaderos Negativos (TN): {tn} - Casos benignos correctamente identificados
        ‚Ä¢ Falsos Positivos (FP): {fp} - Casos benignos incorrectamente clasificados como malignos
        ‚Ä¢ Falsos Negativos (FN): {fn} - Casos malignos incorrectamente clasificados como benignos
        ‚Ä¢ Verdaderos Positivos (TP): {tp} - Casos malignos correctamente identificados
        
        Implicaciones cl√≠nicas:
        ‚Ä¢ Sensibilidad alta: Buena detecci√≥n de casos malignos
        ‚Ä¢ Especificidad alta: Pocas falsas alarmas
        ‚Ä¢ MCC alto: Rendimiento balanceado en ambas clases
        """
        
        self.add_text(confusion_text)
    
    def add_comparison_analysis(self, comparison_data):
        """
        A√±ade an√°lisis de comparaciones estad√≠sticas
        
        Args:
            comparison_data: Datos de comparaciones entre modelos
        """
        if not comparison_data:
            return
        
        self.add_section_title("üìä Comparaci√≥n Estad√≠stica entre Modelos")
        
        self.add_text("An√°lisis utilizando la prueba de McNemar para comparar el rendimiento entre pares de modelos:")
        
        significant_comparisons = []
        non_significant_comparisons = []
        
        for comparison_key, data in comparison_data.items():
            if data and data.get('significant', False):
                significant_comparisons.append((comparison_key, data))
            elif data:
                non_significant_comparisons.append((comparison_key, data))
        
        # Comparaciones significativas
        if significant_comparisons:
            self.add_text("\\nüîç Diferencias Estad√≠sticamente Significativas (p < 0.05):")
            
            for comparison_key, data in significant_comparisons:
                better_model = data['model1'] if data['model1_better'] else data['model2']
                p_value = data['p_value']
                
                comparison_text = f"‚Ä¢ {comparison_key}: {better_model} es significativamente mejor (p = {p_value:.4f})"
                self.add_text(comparison_text)
        
        # Comparaciones no significativas
        if non_significant_comparisons:
            self.add_text("\\n‚öñÔ∏è Sin Diferencias Estad√≠sticamente Significativas:")
            
            for comparison_key, data in non_significant_comparisons:
                p_value = data['p_value']
                comparison_text = f"‚Ä¢ {comparison_key}: No hay diferencia significativa (p = {p_value:.4f})"
                self.add_text(comparison_text)
    
    def add_methodology(self):
        """
        A√±ade secci√≥n de metodolog√≠a
        """
        self.add_section_title("üî¨ Metodolog√≠a de Evaluaci√≥n")
        
        methodology_text = """
        Dataset:
        ‚Ä¢ Fuente: ISIC (International Skin Imaging Collaboration)
        ‚Ä¢ Tipo: Im√°genes reales de lesiones cut√°neas
        ‚Ä¢ Clases: Benigno (0) y Maligno (1)
        ‚Ä¢ Divisi√≥n: 80% entrenamiento, 20% evaluaci√≥n
        
        Modelos Evaluados:
        ‚Ä¢ EfficientNetB4: Red neuronal convolucional pre-entrenada
        ‚Ä¢ ResNet152: Arquitectura residual profunda
        ‚Ä¢ CNN Personalizada: Arquitectura dise√±ada espec√≠ficamente
        
        M√©tricas de Evaluaci√≥n:
        ‚Ä¢ Accuracy: Proporci√≥n de predicciones correctas
        ‚Ä¢ Precision: Proporci√≥n de verdaderos positivos entre predicciones positivas
        ‚Ä¢ Recall (Sensibilidad): Proporci√≥n de verdaderos positivos detectados
        ‚Ä¢ F1-Score: Media arm√≥nica entre precisi√≥n y recall
        ‚Ä¢ MCC: Matthews Correlation Coefficient (m√©trica balanceada)
        ‚Ä¢ AUC-ROC: √Årea bajo la curva ROC
        ‚Ä¢ Especificidad: Proporci√≥n de verdaderos negativos detectados
        
        An√°lisis Estad√≠stico:
        ‚Ä¢ Prueba de McNemar: Para comparar rendimiento entre modelos
        ‚Ä¢ Nivel de significancia: Œ± = 0.05
        ‚Ä¢ Correcci√≥n de continuidad aplicada
        """
        
        self.add_text(methodology_text)
    
    def add_conclusions(self, metrics_data):
        """
        A√±ade conclusiones y recomendaciones
        
        Args:
            metrics_data: Datos de m√©tricas
        """
        self.add_section_title("üí° Conclusiones y Recomendaciones")
        
        # Encontrar mejor modelo
        best_model = max(metrics_data.items(), key=lambda x: x[1]['mcc'])
        model_name, model_data = best_model
        
        # An√°lisis general
        avg_accuracy = np.mean([data['accuracy'] for data in metrics_data.values()])
        avg_mcc = np.mean([data['mcc'] for data in metrics_data.values()])
        
        conclusions_text = f"""
        Resultados Principales:
        ‚Ä¢ Mejor modelo: {model_name} con MCC = {model_data['mcc']:.3f}
        ‚Ä¢ Accuracy promedio: {avg_accuracy:.3f}
        ‚Ä¢ MCC promedio: {avg_mcc:.3f}
        
        Recomendaciones:
        1. Para uso cl√≠nico, considerar el modelo con mayor especificidad para minimizar falsas alarmas
        2. Para screening, priorizar modelos con alta sensibilidad para detectar m√°s casos malignos
        3. El MCC es la m√©trica m√°s apropiada para datasets desbalanceados como este
        4. Se recomienda validaci√≥n adicional con datasets externos
        
        Limitaciones:
        ‚Ä¢ Evaluaci√≥n limitada a dataset ISIC
        ‚Ä¢ Posible sesgo en la selecci√≥n de im√°genes
        ‚Ä¢ Necesidad de validaci√≥n cl√≠nica adicional
        ‚Ä¢ Variabilidad en condiciones de captura de im√°genes
        
        Trabajo Futuro:
        ‚Ä¢ Validaci√≥n cruzada con m√∫ltiples datasets
        ‚Ä¢ An√°lisis de interpretabilidad de los modelos
        ‚Ä¢ Evaluaci√≥n en condiciones cl√≠nicas reales
        ‚Ä¢ Optimizaci√≥n de hiperpar√°metros espec√≠ficos
        """
        
        self.add_text(conclusions_text)
    
    def add_image_if_exists(self, image_path, width=180):
        """
        A√±ade imagen si existe
        
        Args:
            image_path: Ruta a la imagen
            width: Ancho de la imagen en el PDF
        """
        if Path(image_path).exists():
            try:
                self.pdf.image(str(image_path), x=10, w=width)
                self.pdf.ln(width * 0.6)  # Espaciado basado en el ancho
            except Exception as e:
                print(f"‚ùå Error a√±adiendo imagen {image_path}: {e}")
    
    def generate_complete_report(self, metrics_data, comparison_data=None, 
                               title="Reporte Completo de Evaluaci√≥n", 
                               include_images=True, output_filename="reporte_completo.pdf"):
        """
        Genera el reporte completo
        
        Args:
            metrics_data: Datos de m√©tricas de los modelos
            comparison_data: Datos de comparaciones estad√≠sticas
            title: T√≠tulo del reporte
            include_images: Si incluir im√°genes
            output_filename: Nombre del archivo de salida
            
        Returns:
            str: Ruta del archivo generado
        """
        print(f"üìÑ Generando reporte: {title}")
        
        # P√°gina de t√≠tulo
        self.add_title_page(title)
        
        # Nueva p√°gina para contenido
        self.pdf.add_page()
        
        # Resumen ejecutivo
        self.add_section_title("üìã Resumen Ejecutivo")
        best_model = max(metrics_data.items(), key=lambda x: x[1]['mcc'])
        
        executive_summary = f"""
        Se evaluaron {len(metrics_data)} modelos de aprendizaje autom√°tico para la clasificaci√≥n de lesiones cut√°neas 
        utilizando el dataset ISIC. El mejor rendimiento fue obtenido por {best_model[0]} con un MCC de {best_model[1]['mcc']:.3f}.
        
        Total de muestras evaluadas: {list(metrics_data.values())[0]['test_samples']:,}
        Distribuci√≥n: {list(metrics_data.values())[0]['class_distribution'][0]:,} benignos, {list(metrics_data.values())[0]['class_distribution'][1]:,} malignos
        """
        
        self.add_text(executive_summary)
        
        # Tabla de m√©tricas
        self.add_metrics_table(metrics_data)
        
        # An√°lisis del mejor modelo
        self.add_best_model_analysis(metrics_data)
        
        # Nueva p√°gina para comparaciones
        self.pdf.add_page()
        
        # Comparaciones estad√≠sticas
        if comparison_data:
            self.add_comparison_analysis(comparison_data)
        
        # Metodolog√≠a
        self.add_methodology()
        
        # Nueva p√°gina para conclusiones
        self.pdf.add_page()
        
        # Conclusiones
        self.add_conclusions(metrics_data)
        
        # A√±adir im√°genes si est√°n disponibles
        if include_images:
            self.pdf.add_page()
            self.add_section_title("üìä Visualizaciones")
            
            plots_dir = Path("plots")
            if plots_dir.exists():
                # Matriz de confusi√≥n comparativa
                img_path = plots_dir / "confusion_matrices_comparison.png"
                if img_path.exists():
                    self.add_text("Matrices de Confusi√≥n:")
                    self.add_image_if_exists(img_path)
                
                # Comparaci√≥n de m√©tricas
                img_path = plots_dir / "metrics_comparison.png"
                if img_path.exists():
                    self.add_text("Comparaci√≥n de M√©tricas:")
                    self.add_image_if_exists(img_path)
                
                # Curvas ROC
                img_path = plots_dir / "roc_curves.png"
                if img_path.exists():
                    self.add_text("Curvas ROC:")
                    self.add_image_if_exists(img_path)
        
        # Guardar PDF
        output_path = self.output_dir / output_filename
        self.pdf.output(str(output_path))
        
        print(f"‚úÖ Reporte generado: {output_path}")
        
        return str(output_path)
