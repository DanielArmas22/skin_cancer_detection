# app.py
"""
Aplicaci√≥n principal para el sistema de diagn√≥stico de c√°ncer de piel
"""

import streamlit as st
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from PIL import Image

# Importar m√≥dulos propios
from config import initialize_page, MCC_COMPARISON_DATA, PAGE_CONFIG, REAL_TRAINING_METRICS
from model_utils import load_models, predict_image, predict_image_with_debug, predict_image_with_custom_threshold, get_model_info
from preprocessing import preprocess_image
from ui_components import (
    setup_sidebar, display_main_header, display_image_upload_section, display_image_comparison,
    display_diagnosis_results, display_debug_info, display_interpretation, display_model_comparison_table,
    display_consistency_analysis, display_metrics_explanation, display_metrics_in_columns,
    display_mcc_interpretation, display_technical_info, display_medical_disclaimer,
    display_pdf_generation_section, display_mcnemar_results_table, display_statistical_conclusions
)
from data_processor import (
    compare_all_models, analyze_consistency, get_model_metrics,
    create_mcc_comparison_dataframe, format_metrics_for_display
)
# La importaci√≥n de activation_maps ha sido eliminada debido a errores con generate_gradcam
from visualization import (
    plot_confusion_matrix, create_metrics_dashboard, create_advanced_metrics_dashboard,
    create_model_comparison_plots, create_mcc_comparison_chart, create_mcnemar_plot
)
from metrics_calculator import perform_mcnemar_comparisons
from pdf_generator_optimized import generate_pdf_report
from translations import get_available_languages, load_translations


def main():
    """Funci√≥n principal de la aplicaci√≥n"""
    # Inicializar la configuraci√≥n de la p√°gina
    initialize_page()
    
    # Configuraci√≥n inicial para traducciones (espa√±ol por defecto)
    available_languages = get_available_languages()
    t = load_translations('es')
    
    # Cargar modelos entrenados
    @st.cache_resource
    def load_models_cached():
        try:
            models = load_models()
            if not models:
                st.error("‚ùå " + t.get('models_load_error', "No se pudieron cargar los modelos entrenados."))
                st.error("üìù " + t.get('models_folder_check', "Aseg√∫rate de que los archivos .h5 est√©n en la carpeta app/models/"))
                return {}
            return models
        except Exception as e:
            st.error(f"‚ùå {t.get('model_load_exception', 'Error al cargar los modelos')}: {str(e)}")
            return {}

    models = load_models_cached()
    model_names = list(models.keys())

    if not model_names:
        st.error("‚ùå " + t.get('no_models_available', "No hay modelos disponibles. Verifica que los modelos entrenados est√©n en app/models/"))
        st.stop()
    
    # Configurar la barra lateral y obtener la configuraci√≥n del usuario
    sidebar_config = setup_sidebar(models, t, available_languages)
    
    # Actualizar las traducciones seg√∫n el idioma seleccionado
    current_lang_code = available_languages[sidebar_config['lang']]
    t = load_translations(current_lang_code)
    
    # Mostrar encabezado principal
    display_main_header(t)
    
    # Secci√≥n de carga de imagen
    uploaded_file = display_image_upload_section(t)
    
    # Inicializar variables de sesi√≥n si no existen
    if 'uploaded_image' not in st.session_state:
        st.session_state.uploaded_image = None
        st.session_state.processed_image = None
    
    # Procesar nueva imagen si se ha cargado
    if uploaded_file is not None:
        # Guardar la imagen original en la sesi√≥n
        image = Image.open(uploaded_file)
        st.session_state.uploaded_image = image
        
        # Preprocesamiento y guardado en sesi√≥n
        processed_image = preprocess_image(np.array(image))
        st.session_state.processed_image = processed_image
    
    # Procesar la imagen guardada (ya sea nueva o de una carga anterior)
    if st.session_state.uploaded_image is not None:
        # Obtener imagen y su versi√≥n procesada de la sesi√≥n
        image = st.session_state.uploaded_image
        processed_image = st.session_state.processed_image
        
        # Mostrar comparaci√≥n de im√°genes
        display_image_comparison(image, processed_image, t)
        
        # Realizar predicci√≥n con el modelo seleccionado
        st.header("üîç " + t.get('diagnosis_results', "Resultados del Diagn√≥stico"))
        
        with st.spinner(t.get('processing_image', "Analizando imagen...")):
            model = models[sidebar_config['selected_model']]
            
            # Verificamos si necesitamos recalcular el diagn√≥stico o usar el guardado
            recalculate = False
            
            # Inicializar variables de sesi√≥n para diagn√≥stico si no existen
            if 'diagnosis' not in st.session_state:
                st.session_state.diagnosis = None
                st.session_state.confidence_percent = None
                st.session_state.raw_confidence = None
                st.session_state.last_model = None
                st.session_state.last_threshold = None
                recalculate = True
            
            # Recalcular si cambi√≥ el modelo o el umbral de decisi√≥n
            if (st.session_state.last_model != sidebar_config['selected_model'] or 
                st.session_state.last_threshold != sidebar_config['decision_threshold']):
                recalculate = True
            
            if recalculate:
                if sidebar_config['debug_mode']:
                    # Usar funci√≥n de debug
                    diagnosis, confidence_percent, raw_confidence = predict_image_with_debug(model, processed_image)
                    
                    # Mostrar informaci√≥n de debug
                    display_debug_info(processed_image, model, sidebar_config['decision_threshold'], t)
                else:
                    # Usar funci√≥n con umbral personalizado
                    diagnosis, confidence_percent, raw_confidence = predict_image_with_custom_threshold(
                        model, processed_image, threshold=sidebar_config['decision_threshold']
                    )
                
                # Guardamos los resultados en la sesi√≥n
                st.session_state.diagnosis = diagnosis
                st.session_state.confidence_percent = confidence_percent
                st.session_state.raw_confidence = raw_confidence
                st.session_state.last_model = sidebar_config['selected_model']
                st.session_state.last_threshold = sidebar_config['decision_threshold']
            else:
                # Usamos los resultados guardados
                diagnosis = st.session_state.diagnosis
                confidence_percent = st.session_state.confidence_percent
                raw_confidence = st.session_state.raw_confidence
                
                # Si estamos en modo debug, mostramos la informaci√≥n de debug
                if sidebar_config['debug_mode']:
                    display_debug_info(processed_image, model, sidebar_config['decision_threshold'], t)
        
        # Mostrar resultados con mejor dise√±o
        display_diagnosis_results(diagnosis, confidence_percent, raw_confidence, t)
        
        # Interpretaci√≥n de resultados
        display_interpretation(confidence_percent, sidebar_config['confidence_threshold'], diagnosis, t)
        
        # COMPARACI√ìN DE TODOS LOS MODELOS
        st.markdown("---")
        st.subheader("üìä " + t.get('model_comparison', "Comparaci√≥n de Todos los Modelos"))
        st.markdown(t.get('model_comparison_desc', "Resultados de an√°lisis de la misma imagen con diferentes modelos:"))
        
        # Inicializar variable de sesi√≥n para comparaci√≥n si no existe
        if 'comparison_results' not in st.session_state:
            st.session_state.comparison_results = None
            st.session_state.models_hash = None
        
        # Generamos un hash de los modelos para detectar cambios
        current_models_hash = ','.join(sorted(models.keys()))
        
        # Verificar si necesitamos recalcular la comparaci√≥n
        recalculate_comparison = (st.session_state.comparison_results is None or 
                                 st.session_state.models_hash != current_models_hash)
        
        # Realizar predicciones con todos los modelos si es necesario
        if recalculate_comparison:
            with st.spinner(t.get('comparing_models', "Comparando todos los modelos...")):
                comparison_results = compare_all_models(models, processed_image)
                # Guardar resultados en la sesi√≥n
                st.session_state.comparison_results = comparison_results
                st.session_state.models_hash = current_models_hash
        else:
            # Usar resultados guardados
            comparison_results = st.session_state.comparison_results
        
        # Mostrar tabla de comparaci√≥n
        if comparison_results:
            df_comparison = pd.DataFrame(comparison_results)
            display_model_comparison_table(df_comparison)
            
            # Gr√°ficos de comparaci√≥n
            fig_confianza, fig_tiempo = create_model_comparison_plots(df_comparison)
            if fig_confianza:
                st.pyplot(fig_confianza)
            if fig_tiempo:
                st.pyplot(fig_tiempo)
            
            # An√°lisis de consistencia
            display_consistency_analysis(comparison_results, t)
        
        # MATRIZ DE CONFUSI√ìN Y M√âTRICAS
        st.markdown("---")
        confusion_matrix_title = t.get('confusion_matrix_title', "Matriz de Confusi√≥n y M√©tricas")
        st.subheader("üìä " + confusion_matrix_title)
        
        analysis_desc = t.get('model_analysis_description', "An√°lisis detallado del rendimiento del modelo seleccionado")
        st.markdown(f"{analysis_desc}:")
        
        # Inicializar variables de sesi√≥n para m√©tricas si no existen
        if 'metrics_data' not in st.session_state:
            st.session_state.metrics_data = {}
            st.session_state.is_real_data = {}
        
        # Verificar si necesitamos calcular m√©tricas para este modelo
        model_key = sidebar_config['selected_model']
        if model_key not in st.session_state.metrics_data:
            # Obtener m√©tricas del modelo seleccionado
            metrics_data, is_real_data = get_model_metrics(model_key)
            # Guardar en sesi√≥n
            st.session_state.metrics_data[model_key] = metrics_data
            st.session_state.is_real_data[model_key] = is_real_data
        else:
            # Usar m√©tricas guardadas
            metrics_data = st.session_state.metrics_data[model_key]
            is_real_data = st.session_state.is_real_data[model_key]
        
        if is_real_data:
            real_data_msg = t.get('real_data_metrics', "‚úÖ **Datos Reales de Entrenamiento**: Mostrando m√©tricas reales del modelo {model} en el dataset ISIC 2019")
            st.success(real_data_msg.format(model=sidebar_config['selected_model']))
        else:
            simulated_data_msg = t.get('simulated_data_metrics', "‚ö†Ô∏è **Datos Simulados**: Usando datos de ejemplo para demostraci√≥n")
            st.warning(simulated_data_msg)
        
        # Mostrar matriz de confusi√≥n
        col1, col2 = st.columns(2)
        
        with col1:
            confusion_matrix_label = t.get('confusion_matrix_chart', "üéØ Matriz de Confusi√≥n")
            st.markdown(f"**{confusion_matrix_label}**")
            fig_cm = plot_confusion_matrix(metrics_data['confusion_matrix'], sidebar_config['selected_model'])
            if fig_cm:
                st.pyplot(fig_cm)
        
        with col2:
            advanced_metrics_label = t.get('advanced_metrics', "üìà M√©tricas de Rendimiento Avanzadas")
            st.markdown(f"**{advanced_metrics_label}**")
            display_metrics_in_columns(metrics_data, t)
            
            # Interpretaci√≥n de m√©tricas
            interpretation_label = t.get('metrics_interpretation', "üìã Interpretaci√≥n:")
            st.markdown(f"**{interpretation_label}**")
            formatted_metrics = format_metrics_for_display(metrics_data)
            
            # M√©tricas formateadas con valores
            st.markdown(f"""
            - **Accuracy**: {metrics_data['accuracy']*100:.1f}% {t.get('accuracy_explanation', "de las predicciones son correctas")}
            - **Sensitivity**: {metrics_data['sensitivity']*100:.1f}% {t.get('sensitivity_explanation', "de los casos malignos son detectados")}
            - **Specificity**: {metrics_data['specificity']*100:.1f}% {t.get('specificity_explanation', "de los casos benignos son correctamente identificados")}
            - **Precision**: {metrics_data['precision']*100:.1f}% {t.get('precision_explanation', "de los casos clasificados como malignos son realmente malignos")}
            - **F1-Score**: {metrics_data['f1_score']*100:.1f}% {t.get('f1_explanation', "es el balance entre precisi√≥n y sensibilidad")}
            - **MCC**: {metrics_data['mcc']:.3f} {t.get('mcc_explanation', "(Coeficiente de Matthews - balanceado para clases desequilibradas)")}
            """)
        
        # Dashboard completo de m√©tricas
        st.markdown("---")
        dashboard_title = t.get('metrics_dashboard_title', "Dashboard Completo de M√©tricas")
        st.subheader(f"üìä {dashboard_title}")
        
        fig_dashboard = create_metrics_dashboard(metrics_data, sidebar_config['selected_model'])
        if fig_dashboard:
            st.pyplot(fig_dashboard)
        
        # La secci√≥n de visualizaci√≥n de mapas de activaci√≥n ha sido eliminada debido al error:
        # "Error al generar el mapa de activaci√≥n Grad-CAM: The layer sequential has never been called and thus has no defined output."
        
        # Explicaci√≥n de la matriz de confusi√≥n
        display_metrics_explanation(t)
        
        # An√°lisis Estad√≠stico Avanzado
        st.markdown("---")
        statistical_title = t.get('statistical_analysis_title', "An√°lisis Estad√≠stico Avanzado")
        st.subheader(f"üî¨ {statistical_title}")
        
        statistical_desc = t.get('statistical_analysis_description', "Incluyendo Coeficiente de Matthews y Pruebas de McNemar")
        st.markdown(f"{statistical_desc}:")
        
        # Crear dashboard avanzado con MCC
        fig_advanced = create_advanced_metrics_dashboard(metrics_data, sidebar_config['selected_model'])
        if fig_advanced:
            st.pyplot(fig_advanced)
        
        # Tabla de Resumen MCC y Gr√°fico Comparativo
        st.markdown("---")
        mcc_title = t.get('mcc_comparison_title', "Resumen Comparativo de Coeficientes de Matthews (MCC)")
        st.subheader(f"üìä {mcc_title}")
        
        mcc_desc = t.get('mcc_comparison_description', "Comparaci√≥n de todos los modelos basada en el Coeficiente de Matthews")
        st.markdown(f"{mcc_desc}:")
        
        # Crear DataFrame para la tabla
        df_mcc = create_mcc_comparison_dataframe(MCC_COMPARISON_DATA)
        
        # Mostrar tabla con formato mejorado
        col1, col2 = st.columns([3, 2])
        
        with col1:
            mcc_table_title = t.get('mcc_table_title', "üìã Tabla de Resumen - Coeficientes de Matthews")
            st.markdown(f"**{mcc_table_title}**")
            st.dataframe(df_mcc, use_container_width=True)
        
        with col2:
            display_mcc_interpretation(MCC_COMPARISON_DATA, t)
        
        # Gr√°fico MCC
        fig_mcc = create_mcc_comparison_chart(MCC_COMPARISON_DATA)
        if fig_mcc:
            st.pyplot(fig_mcc)
        
        # An√°lisis Estad√≠stico - Pruebas de McNemar
        st.markdown("---")
        mcnemar_title = t.get('mcnemar_tests_title', "Pruebas Estad√≠sticas de McNemar")
        st.subheader(f"üî¨ {mcnemar_title}")
        
        mcnemar_desc = t.get('mcnemar_description', "Comparaci√≥n estad√≠stica entre modelos")
        st.markdown(f"{mcnemar_desc}:")
        
        # Generar resultados de McNemar usando los datos de MCC para los nombres de modelos
        mcnemar_results = perform_mcnemar_comparisons(MCC_COMPARISON_DATA)
        
        # Mostrar tabla de resultados
        display_mcnemar_results_table(mcnemar_results, t)
        
        # Gr√°fico de p-valores
        fig_mcnemar = create_mcnemar_plot(mcnemar_results)
        if fig_mcnemar:
            st.pyplot(fig_mcnemar)
        
        # Conclusiones estad√≠sticas
        display_statistical_conclusions(mcnemar_results, t)
        
        # Generar reporte PDF
        generate_pdf = display_pdf_generation_section(t)
        
        if generate_pdf:
            generating_pdf_msg = t.get('generating_pdf', "Generando reporte PDF...")
            with st.spinner(generating_pdf_msg):
                # Preparar datos para el PDF
                confidence_comparison_label = t.get('confidence_comparison_plot', "Comparacion de Confianza")
                inference_speed_label = t.get('inference_speed_plot', "Velocidad de Inferencia")
                mcc_comparative_label = t.get('mcc_comparative_plot', "MCC Comparativo")
                mcnemar_pvalues_label = t.get('mcnemar_pvalues_plot', "McNemar P-valores")
                
                plots_data = {
                    'confusion_matrix': fig_cm if 'fig_cm' in locals() else None,
                    'metrics_dashboard': fig_dashboard if 'fig_dashboard' in locals() else None,
                    'advanced_dashboard': fig_advanced if 'fig_advanced' in locals() else None,
                    'comparison_plots': {
                        confidence_comparison_label: fig_confianza if 'fig_confianza' in locals() else None,
                        inference_speed_label: fig_tiempo if 'fig_tiempo' in locals() else None,
                        mcc_comparative_label: fig_mcc if 'fig_mcc' in locals() else None,
                        mcnemar_pvalues_label: fig_mcnemar if 'fig_mcnemar' in locals() else None
                    }
                }
                
                # Generar PDF
                generate_pdf_report(
                    image=image,
                    diagnosis=diagnosis,
                    confidence_percent=confidence_percent,
                    raw_confidence=raw_confidence,
                    model_name=sidebar_config['selected_model'],
                    model_info=get_model_info(models[sidebar_config['selected_model']]),
                    comparison_results=comparison_results,
                    translations=t,
                    confidence_threshold=sidebar_config['confidence_threshold'],
                    metrics_data=metrics_data,
                    plots_data=plots_data
                )
        
        # Informaci√≥n t√©cnica
        # Crear configuraci√≥n para informaci√≥n t√©cnica
        technical_config = {
            "DATASET_NAME": PAGE_CONFIG.get("dataset_name", "ISIC 2019"),
            "DATASET_SIZE": "25,331",
            "MODEL_ACCURACY": f"~{REAL_TRAINING_METRICS[sidebar_config['selected_model']]['accuracy']*100:.1f}%",
            "OPTIMIZATION_TARGET": "c√°ncer de piel"
        }
        display_technical_info(get_model_info(models[sidebar_config['selected_model']]), t, technical_config)
        
        # Advertencia m√©dica
        display_medical_disclaimer(t)


if __name__ == "__main__":
    main()
